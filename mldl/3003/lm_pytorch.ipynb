{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fun with language modelling\n",
    "\n",
    "Если вы пропустили лекцию, то посмотрите слайды к ней — они где-то есть. Также полезно почитать:\n",
    "\n",
    "* [Unreasonable effectiveness of RNN](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) (Andrej Karpathy)\n",
    "* [Официальный пример от PyTorch](https://github.com/pytorch/examples/tree/master/word_language_model)\n",
    "\n",
    "Рекомендуется заранее всё прочитать, чтобы понять, что от вас хотят. При желании, можете переписать всё так, как подсказывает ваше сердце.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = 40  # предложения с меньшим количеством символов не будут рассматриваться\n",
    "max_len = 150 # предложения с большим количеством символов будут обрезаться"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг (3 балла)\n",
    "\n",
    "Возьмите какие-нибудь сырые данные. Википедия, «Гарри Поттер», «Игра Престолов», тексты Монеточки, твиты Тинькова — что угодно.\n",
    "\n",
    "Для простоты будем делать char-level модель. Выкиньте из текстов все ненужные символы (можете оставить только алфавит и, пунктуацию). Сопоставьте всем различным символам свой номер. Удобно это хранить просто в питоновском словаре (`char2idx`). Для генерации вам потребуется ещё и обратный словарь (`idx2char`). Вы что-то такое должны были писать на вступительной — можете просто переиспользовать код оттуда.\n",
    "\n",
    "Заранее зарезервируйте айдишники под служебные символы: `<START>`, `<END>`, `<PAD>`, `<UNK>`.\n",
    "\n",
    "Клёво будет написать отдельный класс, который делает токенизацию и детокенизацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, data):\n",
    "        \n",
    "        self.char2idx = {}\n",
    "        self.idx2char = {}\n",
    "        self.char2idx\n",
    "        ptr_idx = 0\n",
    "        for s in data:\n",
    "            for c in s:\n",
    "                if c not in self.char2idx:\n",
    "                    self.char2idx[c] = ptr_idx\n",
    "                    self.idx2char[ptr_idx] = c\n",
    "                    ptr_idx += 1\n",
    "    \n",
    "    def tokenize(self, sequence):\n",
    "        # выполните какой-то базовый препроцессинг\n",
    "        # например, оставьте только алфавит и пунктуацию\n",
    "        return [self.char2idx[char] for char in sequence if char in self.char2idx]\n",
    "    \n",
    "    def detokenize(self, sequence):\n",
    "        return ''.join([self.idx2char[idx.item()] for idx in sequence])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        print('here')\n",
    "        self.data = []\n",
    "        with open('book.txt', 'r') as f:\n",
    "            data = f.readlines()[:1000]\n",
    "            self.vocab = Vocab(data)\n",
    "        cur = torch.LongTensor(torch.zeros([max_len, len(self.vocab)], dtype=torch.int64))\n",
    "        ptr = 0\n",
    "        with open('book.txt', 'r') as f:\n",
    "            data = f.readlines()[:1000]\n",
    "            for s in data:\n",
    "                for c in s:\n",
    "                    cur[ptr][self.vocab.tokenize(c)[0]] = 1\n",
    "                    ptr += 1\n",
    "                    if ptr == max_len:\n",
    "                        self.data.append(cur)\n",
    "                        ptr = 0\n",
    "                        cur = torch.LongTensor(torch.zeros([max_len, len(self.vocab)], dtype=torch.int64))\n",
    "        print('here')\n",
    "        # обучите вокаб\n",
    "        print('here')\n",
    "        # разделите данные на отдельные сэмплы для обучения\n",
    "        # (просто список из сырых строк)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        sample = torch.LongTensor(sample) # сконвертируйте в LongTensor\n",
    "        target = torch.LongTensor(torch.zeros(1, len(self.vocab), dtype=torch.int64))\n",
    "        target = torch.cat([target, sample]) # нужно предсказать эту же последовательность со сдвигом 1\n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если у вас какой-то большой массив текста (скажем, статьи Википедии), вы можете просто нарезать из него кусочки фиксированной длины и так их подавать в модель.\n",
    "\n",
    "Если же вы хотите приключений, то можно разбить этот текст на предложения (`nltk.sent_tokenize`), и тогда все примеры будут разной длины. По соображениям производительности, вы не хотите использовать самые длинные и самые короткие сэмплы, поэтому имеет смысл обрезать их по длине."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьём на обучение и валидацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "here\n",
      "here\n",
      "388\n"
     ]
    }
   ],
   "source": [
    "dataset = TextDataset('./book.txt')\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "print(len(train_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = train_set[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель (3 балла)\n",
    "\n",
    "Примерно такое должно зайти:\n",
    "\n",
    "* Эмбеддинг\n",
    "* LSTM / GRU\n",
    "* Линейный слой\n",
    "* Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout, tie_weights):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        \n",
    "        if tie_weights:\n",
    "            # \"Using the Output Embedding to Improve Language Models\" (Press & Wolf 2016)\n",
    "            # https://arxiv.org/abs/1608.05859\n",
    "            assert hidden_dim == embedding_dim\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.nhid = hidden_dim\n",
    "        self.nlayers = num_layers\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # начальный хидден должен быть нулевой\n",
    "        # (либо хоть какой-то константный для всего обучения)\n",
    "        return torch.zeros(batch_size, hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "lr = 1e-3 \n",
    "batch_size = 64\n",
    "hidden_dim = 128\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = LM(\n",
    "    vocab_size = len(dataset.vocab),\n",
    "    embedding_dim = hidden_dim,\n",
    "    hidden_dim = hidden_dim,\n",
    "    num_layers = max_len,\n",
    "    dropout = 0.1,\n",
    "    tie_weights= True\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test = DataLoader(test_set, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 64, 79])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at /opt/conda/conda-bld/pytorch-cpu_1544217717168/work/aten/src/TH/THGeneral.cpp:201",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8058b6918a51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# 2. Прогоните данные через модель, получите предсказания на каждом токене\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-6774b36f6dc1>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    747\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[1;32m    748\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 0GB. Buy new RAM! at /opt/conda/conda-bld/pytorch-cpu_1544217717168/work/aten/src/TH/THGeneral.cpp:201"
     ]
    }
   ],
   "source": [
    "for e in range(epochs):\n",
    "    for x, y in train:\n",
    "        x = x.transpose(0, 1)\n",
    "        print(x.shape)\n",
    "        model.train()\n",
    "        \n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        # 0. Распакуйте данные на нужное устройство\n",
    "        # 1. Инициилизируйте hidden\n",
    "        hidden = model.init_hidden(len(x))\n",
    "        # 2. Прогоните данные через модель, получите предсказания на каждом токене\n",
    "        output, hidden = model(x, hidden)\n",
    "        print(output)\n",
    "        break\n",
    "        # 3. Посчитайте лосс (maxlen независимых классификаций) и сделайте backward()\n",
    "#         for i in range(max_len):\n",
    "#             loss = criterion.forward(y[i])\n",
    "        # 4. Клипните градиенты -- у RNN-ок с этим часто бывают проблемы\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.2)\n",
    "        # 5. Залоггируйте лосс куда-нибудь\n",
    "        \n",
    "        optimizer.step()\n",
    "    \n",
    "    for x, y in test:\n",
    "        model.eval()\n",
    "         \n",
    "        hidden = model.init_hidden(len(x))\n",
    "        output, hidden = model(x, hidden)\n",
    "        # 3. Посчитайте лосс (maxlen независимых классификаций) и сделайте backward()\n",
    "        # 4. Клипните градиенты -- у RNN-ок с этим часто бывают проблемы\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.2)\n",
    "        # 5. Залоггируйте лосс куда-нибудь\n",
    "        \n",
    "        # сдесь нужно сделать то же самое, только без backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Спеллчекер (3 балла)\n",
    "\n",
    "Из языковой модели можно сделать простенький спеллчекер: можно визуализировать лоссы на каждом символе (либо какой-нибудь другой показатель неуверенности).\n",
    "\n",
    "Бонус: можете усреднить перплексии по словам и выделять их, а не отдельные символы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "def print_colored(sequence, intensities, delimeter=''):\n",
    "    html = delimeter.join([\n",
    "        f'<span style=\"background: rgb({255}, {255-x}, {255-x})\">{c}</span>'\n",
    "        for c, x in zip(sequence, intensities) \n",
    "    ])\n",
    "    display(HTML(html))\n",
    "\n",
    "print_colored('Налейте мне экспрессо'.split(), [0, 0, 100], ' ')\n",
    "\n",
    "sequence = 'Эту домашку нужно сдать втечении двух недель'\n",
    "intensities = [0]*len(sequence)\n",
    "intensities[25] = 50\n",
    "intensities[26] = 60\n",
    "intensities[27] = 70\n",
    "intensities[31] = 150\n",
    "print_colored(sequence, intensities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spellcheck(sequence):\n",
    "    model.eval()\n",
    "    \n",
    "    # векторизуйте sequence; паддинги делать не нужно\n",
    "    sequence = ...\n",
    "    \n",
    "    # прогоните модель и посчитайте лосс, но не усредняйте\n",
    "    # с losses можно что-нибудь сделать для визуализации; например, в какую-нибудь степень возвести\n",
    "    losses = # ...\n",
    "    \n",
    "    print_colored(sequence, np.array(spellcheck_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = ['В этом претложениии очен много очепяток.', \n",
    "             'Здесь появилась лишнняя буква.', \n",
    "             'В этом предложении все нормально.', \n",
    "             'Чтонибудь пишеться чериз дефис.', \n",
    "             'Слова нрпдзх не сущесдвует.']\n",
    "\n",
    "for sequence in sequences:\n",
    "    spellcheck(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация предложений (3 балла)\n",
    "\n",
    "* Поддерживайте hidden state при генерации. Не пересчитывайте ничего больше одного раза.\n",
    "* Прикрутите температуру: это когда при сэмплировании все логиты (то, что перед софтмаксом) делятся на какое-то число (по умолчанию 1, тогда ничего не меняется). Температура позволяет делать trade-off между разнообразием и правдоподобием (подробнее — см. блог Карпатого).\n",
    "* Ваша реализация должна уметь принимать строку seed — то, с чего должно начинаться сгенерированная строка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(num_tokens, seed=\"\", temperature=1.0):\n",
    "    model.eval()\n",
    "        \n",
    "    hidden = model.init_hidden()\n",
    "    input = # векторизауйте seed, чтобы на первом этап получить нужный hidden    \n",
    "    \n",
    "    continuation = ''\n",
    "    \n",
    "    for _ in range(num_tokens)\n",
    "        output, hidden = model(input, hidden)\n",
    "        \n",
    "        token_probas = output.squeeze().div(temperature).exp().cpu()\n",
    "        token = torch.multinomial(token_probas, 1)[0]\n",
    "        \n",
    "        continuation += # допишите соответствующий символ\n",
    "        input = # обновите input\n",
    "    \n",
    "    return continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginnings = ['Шел медведь по лесу', \n",
    "              'Встретились англичанин, американец и русский. Англичанин говорит:',\n",
    "              'Так вот, однажды качки решили делать ремонт',\n",
    "              'Поручик Ржевский был',\n",
    "              'Идёт Будда с учениками по дороге',\n",
    "              'Мюллер: Штирлиц, где вы были в 1938 году?',\n",
    "              'Засылают к нам американцы шпиона под видом студента',\n",
    "              'Подъезжает электричка к Долгопе:']\n",
    "\n",
    "for beginning in beginnings:\n",
    "    print(f'{beginning}... {sample(10, beginning)})\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
