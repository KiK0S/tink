{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNxseYLzzvCv"
   },
   "source": [
    "# Seminar: simple question answering\n",
    "![img](https://recruitlook.com/wp-content/uploads/2015/01/questionanswer3.jpg)\n",
    "\n",
    "Today we're going to build a retrieval-based question answering model with metric learning models.\n",
    "\n",
    "_this seminar is based on original notebook by [Oleg Vasilev](https://github.com/Omrigan/)_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUxx5lPpzvCx"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "colab_type": "code",
    "id": "k5tlHnRNz1gh",
    "outputId": "d22ecffb-f60b-4d42-c43d-be26d76b203c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-04-19 15:22:58--  https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/week11_dssm/utils.py\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9814 (9.6K) [text/plain]\n",
      "Saving to: ‘utils.py.2’\n",
      "\n",
      "\r",
      "utils.py.2            0%[                    ]       0  --.-KB/s               \r",
      "utils.py.2          100%[===================>]   9.58K  --.-KB/s    in 0s      \n",
      "\n",
      "2019-04-19 15:22:58 (97.3 MB/s) - ‘utils.py.2’ saved [9814/9814]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/week11_dssm/utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "colab_type": "code",
    "id": "L-Rp5Msv0BNf",
    "outputId": "88f46fb0-d732-4783-b5b8-e5b146dc23d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OxgOGXMbzvC2"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "Today's data is Stanford Question Answering Dataset (SQuAD). Given a paragraph of text and a question, our model's task is to select a snippet that answers the question.\n",
    "\n",
    "We are not going to solve the full task today. Instead, we'll train a model to __select the sentence containing answer__ among several options.\n",
    "\n",
    "As usual, you are given an utility module with data reader and some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "US39zXJlzvC2"
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "!wget https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json -O squad-v2.0.json 2> log\n",
    "# backup download link: https://www.dropbox.com/s/q4fuihaerqr0itj/squad.tar.gz?dl=1\n",
    "train, test = utils.build_dataset('./squad-v2.0.json', tokenized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "pF7j-vCpzvC6",
    "outputId": "b0859c08-0168-4af3-e140-6fd4a0a15aa6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i ... i ' m the monument to all your sins .\""
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the data comes pre-tokenized with this simple tokenizer:\n",
    "utils.tokenize(\"I... I'm the monument to all your sins.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 519
    },
    "colab_type": "code",
    "id": "MNr15FbFzvC9",
    "outputId": "9c52fae3-5cb1-44d7-a231-bd8e79c507f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUESTION where did beyonce get her name from ? \n",
      "\n",
      "[ ] beyoncé giselle knowles was born in houston , texas , to celestine ann \" tina \" knowles ( née beyincé ), a hairdresser and salon owner , and mathew knowles , a xerox sales manager .\n",
      "[v] beyoncé ' s name is a tribute to her mother ' s maiden name .\n",
      "[ ] beyoncé ' s younger sister solange is also a singer and a former member of destiny ' s child .\n",
      "[ ] mathew is african - american , while tina is of louisiana creole descent ( with african , native american , french , cajun , and distant irish and spanish ancestry ).\n",
      "[ ] through her mother , beyoncé is a descendant of acadian leader joseph broussard .\n",
      "[ ] she was raised in a methodist household .\n"
     ]
    }
   ],
   "source": [
    "pid, question, options, correct_indices, wrong_indices = train.iloc[40]\n",
    "print('QUESTION', question, '\\n')\n",
    "for i, cand in enumerate(options):\n",
    "    print(['[ ]', '[v]'][i in correct_indices], cand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "cyeFnX6eDuEJ",
    "outputId": "f73a857e-994a-4a9a-d76b-a28784c2d2c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            when did beyonce start becoming popular ?\n",
       "1    what areas did beyonce compete in when she was...\n",
       "2    when did beyonce leave destiny ' s child and b...\n",
       "3         in what city and state did beyonce grow up ?\n",
       "4          in which decade did beyonce become famous ?\n",
       "Name: question, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['question'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vvGDZJUGoZK"
   },
   "outputs": [],
   "source": [
    "words_o = [w.split() for s in train['options'].values for w in s]\n",
    "words_o = [w for v in words_o for w in v if w not in '?\\'&,.;']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8dQ3ufZlFC18"
   },
   "outputs": [],
   "source": [
    "words_q = [s.split() for s in train['question'].values]\n",
    "words_q = [w for v in words_q for w in v if w not in '?\\'&,.;']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKnrxjsKzvDA"
   },
   "source": [
    "### Tokens & vocabularies\n",
    "\n",
    "The procedure here is very similar to previous nlp weeks: preprocess text into tokens, create dictionaries, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Oh7zJoJzvDC"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "#Dictionary of {token : count}\n",
    "words = []\n",
    "for s in train['question']:\n",
    "  for w in utils.tokenize(s).split():\n",
    "    words.append(w)\n",
    "    \n",
    "for b in train['options']:\n",
    "  for s in b: \n",
    "    for w in utils.tokenize(s).split():\n",
    "      words.append(w)\n",
    "# compute counts for each token; use token_counts;\n",
    "# count BOTH in train['question'] and in train['options']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WUt1CTEGIhBt"
   },
   "outputs": [],
   "source": [
    "token_counts = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 311
    },
    "colab_type": "code",
    "id": "miZzEh1UzvDG",
    "outputId": "b6da2a07-e4cb-4523-c0f0-4f6f7d9c0a33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 9042915\n",
      "Most common: [('the', 597790), (',', 438053), ('.', 304508), ('of', 300056), ('and', 231619)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total tokens:\", sum(token_counts.values()))\n",
    "print(\"Most common:\", token_counts.most_common(5))\n",
    "assert 9000000 < sum(token_counts.values()) < 9100000, \"are you sure you counted all unique tokens in questions and options?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mlw19UFFzvDJ"
   },
   "source": [
    "We shall only keep tokens that are present at least 4 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "colab_type": "code",
    "id": "aNvC_4tDzvDK",
    "outputId": "21f169ec-a6c0-4e5c-f0ab-d7088a17d232"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens left: 55940\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 5\n",
    "\n",
    "tokens = [w for w, c in token_counts.items() if c >= MIN_COUNT] \n",
    "tokens = [\"_PAD_\", \"_UNK_\"] + tokens\n",
    "print(\"Tokens left:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A9NK0qpRzvDM"
   },
   "outputs": [],
   "source": [
    "# a dictionary from token to it's index in tokens\n",
    "token_to_id = {t:i for i,t in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAWawL47zvDP"
   },
   "outputs": [],
   "source": [
    "assert token_to_id['me'] != token_to_id['woods']\n",
    "assert token_to_id[tokens[42]]==42\n",
    "assert len(token_to_id)==len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Xn5NzZ0zvDS"
   },
   "outputs": [],
   "source": [
    "PAD_ix = token_to_id[\"_PAD_\"]\n",
    "UNK_ix = token_to_id['_UNK_']\n",
    "\n",
    "#good old as_matrix for the third time\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    if isinstance(sequences[0], (str, bytes)):\n",
    "        sequences = [utils.tokenize(s).split() for s in sequences]\n",
    "        \n",
    "    max_len = max_len or max(map(len,sequences))\n",
    "    \n",
    "    matrix = np.zeros((len(sequences), max_len), dtype='int32') + PAD_ix\n",
    "    for i, seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_ix) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "colab_type": "code",
    "id": "wK25QWBNzvDV",
    "outputId": "54c97408-45d1-4aff-91c6-54ad7dc00f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11406    69  2010  7828  7504    19   977 12465     1     0]\n",
      " [  197    19  2538    37  6879    49   652  6306 25055   340]]\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "test = as_matrix([\"Definitely, thOsE tokens areN'T LowerCASE!!\", \"I'm the monument to all your sins.\"])\n",
    "print(test)\n",
    "# assert test.shape==(2,8)\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T058hedOzvDX"
   },
   "source": [
    "### Data sampler\n",
    "\n",
    "Our model trains on triplets: $<query, answer^+, answer^->$\n",
    "\n",
    "For your convenience, we've implemented a function that samples such triplets from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eg4xlH_6zvDX"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "lines_to_tensor = lambda lines, max_len=None: torch.tensor(\n",
    "    as_matrix(lines, max_len=max_len), dtype=torch.int64)\n",
    "\n",
    "def iterate_minibatches(data, batch_size, shuffle=True, cycle=False):\n",
    "    \"\"\"\n",
    "    Generates minibatches of triples: {questions, correct answers, wrong answers}\n",
    "    If there are several wrong (or correct) answers, picks one at random.\n",
    "    \"\"\"\n",
    "    indices = np.arange(len(data))\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(indices)\n",
    "        for batch_start in range(0, len(indices), batch_size):\n",
    "            batch_indices = indices[batch_start: batch_start + batch_size]\n",
    "            batch = data.iloc[batch_indices]\n",
    "            questions = batch['question'].values\n",
    "            correct_answers = np.array([\n",
    "                row['options'][random.choice(row['correct_indices'])]\n",
    "                for i, row in batch.iterrows()\n",
    "            ])\n",
    "            wrong_answers = np.array([\n",
    "                row['options'][random.choice(row['wrong_indices'])]\n",
    "                for i, row in batch.iterrows()\n",
    "            ])\n",
    "\n",
    "            yield {\n",
    "                'questions' : lines_to_tensor(questions),\n",
    "                'correct_answers': lines_to_tensor(correct_answers),\n",
    "                'wrong_answers': lines_to_tensor(wrong_answers),\n",
    "            }\n",
    "        if not cycle:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 676
    },
    "colab_type": "code",
    "id": "KuWxum9izvDZ",
    "outputId": "e99b9826-f0c0-4b49-fbbb-9095fc8a8a1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'questions': tensor([[   45,     3,    37,  7249,  1724,    49,  2334,  2656,     8],\n",
      "        [    9,    36,     3, 10074,   959,     8,     0,     0,     0]]), 'correct_answers': tensor([[ 6533, 12573,   161, 11291,  2373,  1191,    90,    37,    68,   145,\n",
      "          3143,   480,   859,    49,  2334,  2656,    55,    37,  6533, 15096,\n",
      "            36,   340,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [ 1104,  1010,   119,    66,    37, 14091,    66, 32884,    69,  4623,\n",
      "           770,   523,    37,    53, 32884,   199,    49, 32885,  6533, 22358,\n",
      "            22,    37,   776,   119,    49,   221,    49,    17, 32884,    22,\n",
      "           244,   161,   310,   155,  6979,  2162,   340]]), 'wrong_answers': tensor([[   37,  1296,  1872,   418,   772, 27599,  2655, 13762,  1472,  6213,\n",
      "            69, 19294,    37,  4172,    66,  2655,  2656,    69,  1497,    37,\n",
      "          8336,   340,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [   67, 10074,    19,    20,   976,    12,  8369,    69, 14107,   948,\n",
      "            37, 10115,  5936,  3525,    22,    12,  1391,    37,   800, 12725,\n",
      "            22,   395,   935,    24,  2354,    66,  6060,    22, 18373,   482,\n",
      "         12602,    37,  4172,    66,    37,    31,    69,    22,    30,  4467,\n",
      "            49,  2251,   120,    66,    37,   686, 10115,  5936,   340]])}\n"
     ]
    }
   ],
   "source": [
    "dummy_batch = next(iterate_minibatches(train.sample(2), 3))\n",
    "print(dummy_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iBHPzOdZzvDb"
   },
   "source": [
    "### Building the model (3 points)\n",
    "\n",
    "Our goal for today is to build a model that measures similarity between question and answer. In particular, it maps both question and answer into fixed-size vectors such that:\n",
    "\n",
    "Our model is a pair of $V_q(q)$ and $V_a(a)$ - networks that turn phrases into vectors. \n",
    "\n",
    "__Objective:__ Question vector $V_q(q)$ should be __closer__ to correct answer vectors $V_a(a^+)$ than to incorrect ones $V_a(a^-)$ .\n",
    "\n",
    "Both vectorizers can be anything you wish. For starters, let's use a convolutional network with global pooling and a couple of dense layers on top.\n",
    "\n",
    "It is perfectly legal to share some layers between vectorizers, but make sure they are at least a little different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HItbkJWNzvDc"
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class GlobalMaxPooling(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.max(dim=self.dim)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0dOYRxYlzvDd"
   },
   "outputs": [],
   "source": [
    "# we might as well create a global embedding layer here\n",
    "\n",
    "GLOBAL_EMB = nn.Embedding(len(tokens), 64, padding_idx=PAD_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JlcobjCcfvro"
   },
   "outputs": [],
   "source": [
    "q = QuestionVectorizer(n_tokens=len(tokens), out_size=64, use_global_emb=True)\n",
    "q.forward(text_ix)\n",
    "\n",
    "q(text_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aO0LtQmUzvDf"
   },
   "outputs": [],
   "source": [
    "class QuestionVectorizer(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64, use_global_emb=True):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for questions.\n",
    "        Use any combination of layers you want to encode a variable-length input \n",
    "        to a fixed-size output vector\n",
    "        \n",
    "        If use_global_emb is True, use GLOBAL_EMB as your embedding layer\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        if use_global_emb:\n",
    "            self.emb = GLOBAL_EMB\n",
    "        else:\n",
    "            self.emb = nn.Embedding(len(tokens), 64, padding_idx=PAD_ix)\n",
    "            \n",
    "        self.rnn = torch.nn.RNN(inp)\n",
    "\n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        <YOUR CODE>\n",
    "        return <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LLdm9gekzvDg"
   },
   "outputs": [],
   "source": [
    "class AnswerVectorizer(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), out_size=64, use_global_emb=True):\n",
    "        \"\"\" \n",
    "        A simple sequential encoder for answers.\n",
    "        x -> emb -> conv -> global_max -> relu -> dense\n",
    "        \n",
    "        If use_global_emb is True, use GLOBAL_EMB as your embedding layer\n",
    "        \"\"\"\n",
    "        super(self.__class__, self).__init__()\n",
    "        if use_global_emb:\n",
    "            self.emb = GLOBAL_EMB\n",
    "        else:\n",
    "            self.emb = <YOUR CODE>\n",
    "            \n",
    "        <YOUR CODE>\n",
    "        \n",
    "    def forward(self, text_ix):\n",
    "        \"\"\"\n",
    "        :param text_ix: int64 Variable of shape [batch_size, max_len]\n",
    "        :returns: float32 Variable of shape [batch_size, out_size]\n",
    "        \"\"\"\n",
    "        <YOUR CODE>\n",
    "        return <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xm7FDKkzzvDh"
   },
   "outputs": [],
   "source": [
    "for vectorizer in [QuestionVectorizer(out_size=100), AnswerVectorizer(out_size=100)]:\n",
    "    print(\"Testing %s ...\" % vectorizer.__class__.__name__)\n",
    "    dummy_x = Variable(torch.LongTensor(test))\n",
    "    dummy_v = vectorizer(dummy_x)\n",
    "\n",
    "    assert isinstance(dummy_v, Variable)\n",
    "    assert tuple(dummy_v.shape) == (dummy_x.shape[0], 100)\n",
    "\n",
    "    del vectorizer\n",
    "    print(\"Seems fine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CKfT16nQzvDj"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "question_vectorizer = QuestionVectorizer()\n",
    "answer_vectorizer = AnswerVectorizer()\n",
    "\n",
    "opt = torch.optim.Adam(chain(question_vectorizer.parameters(),\n",
    "                             answer_vectorizer.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "Upf0vSJqzvDl"
   },
   "source": [
    "### Training: loss function (3 points)\n",
    "We want our vectorizers to put correct answers closer to question vectors and incorrect answers farther away from them. One way to express this is to use is Pairwise Hinge Loss _(aka Triplet Loss)_. \n",
    "\n",
    "$$ L = \\frac 1N \\underset {q, a^+, a^-} \\sum max(0, \\space \\delta - sim[V_q(q), V_a(a^+)] + sim[V_q(q), V_a(a^-)] )$$\n",
    "\n",
    ", where\n",
    "* sim[a, b] is some similarity function: dot product, cosine or negative distance\n",
    "* δ - loss hyperparameter, e.g. δ=1.0. If sim[a, b] is linear in b, all δ > 0 are equivalent.\n",
    "\n",
    "\n",
    "This reads as __Correct answers must be closer than the wrong ones by at least δ.__\n",
    "\n",
    "![img](https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/resources/margin.png)\n",
    "<center>_image: question vector is green, correct answers are blue, incorrect answers are red_</center>\n",
    "\n",
    "\n",
    "Note: in effect, we train a Deep Semantic Similarity Model [DSSM](https://www.microsoft.com/en-us/research/project/dssm/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SlCQSkPxzvDm"
   },
   "outputs": [],
   "source": [
    "def compute_loss(anchors, positives, negatives, delta=1):\n",
    "    \"\"\" \n",
    "    Compute the triplet loss:\n",
    "    \n",
    "    max(0, delta + sim(anchors, negatives) - sim(anchors, positives))\n",
    "    \n",
    "    where sim is a dot-product between vectorized inputs\n",
    "    \n",
    "    \"\"\"\n",
    "    <YOUR CODE>\n",
    "    return <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHdS0yjezvDo"
   },
   "outputs": [],
   "source": [
    "def compute_recall(anchors, positives, negatives, delta=1):\n",
    "    \"\"\"\n",
    "    Compute the probability (ratio) at which sim(anchors, negatives) is greater than sim(anchors, positives)\n",
    "    \"\"\"\n",
    "    <YOUR CODE>\n",
    "    return <YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mRHjzQztzvDp"
   },
   "outputs": [],
   "source": [
    "print(compute_loss(_dummy_anchors, _dummy_positives, _dummy_negatives))\n",
    "print(compute_recall(_dummy_anchors, _dummy_positives, _dummy_negatives))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAxDxnGrzvDr"
   },
   "source": [
    "### Training loop (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "BdbIN9MFzvDr"
   },
   "source": [
    "For a difference, we'll ask __you__ to implement training loop this time.\n",
    "\n",
    "Here's a sketch of one epoch:\n",
    "1. iterate over __`batches_per_epoch`__ batches from __`train_data`__ with __`iterate_minibatches`__\n",
    "    * Compute loss, backprop, optimize\n",
    "    * Compute and accumulate recall\n",
    "    \n",
    "2. iterate over __`batches_per_epoch`__ batches from __`val_data`__\n",
    "    * Compute and accumulate recall\n",
    "    \n",
    "3. print stuff :)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PaXlxhxVzvDs"
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "max_len = 100\n",
    "batch_size = 32\n",
    "batches_per_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tet-fHLqzvDt"
   },
   "outputs": [],
   "source": [
    "<YOUR CODE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMJRQQ2TzvDu"
   },
   "outputs": [],
   "source": [
    "# in fact, a lot of your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hnRFaPT-zvDw"
   },
   "outputs": [],
   "source": [
    "# a whole lot of it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rD3deRdzvDx"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "Let's see how our model performs on actual question answering. You will score answer candidates with your model and select the most appropriate one.\n",
    "\n",
    "__Your goal__ is to obtain accuracy of at least above 50%. Beating 65% in this notebook yields bonus points :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ziEOqbLgzvDx"
   },
   "outputs": [],
   "source": [
    "# optional: prepare some functions here\n",
    "# <...>\n",
    "\n",
    "def select_best_answer(question, possible_answers):\n",
    "    \"\"\"\n",
    "    Predicts which answer best fits the question\n",
    "    :param question: a single string containing a question\n",
    "    :param possible_answers: a list of strings containing possible answers\n",
    "    :returns: integer - the index of best answer in possible_answer\n",
    "    \"\"\"\n",
    "    <YOUR CODE>\n",
    "    return <...>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PVgvCo1ozvDy"
   },
   "outputs": [],
   "source": [
    "predicted_answers = [\n",
    "    select_best_answer(question, possible_answers)\n",
    "    for i, (question, possible_answers) in tqdm(test[['question', 'options']].iterrows(), total=len(test))\n",
    "]\n",
    "\n",
    "accuracy = np.mean([\n",
    "    answer in correct_ix\n",
    "    for answer, correct_ix in zip(predicted_answers, test['correct_indices'].values)\n",
    "])\n",
    "print(\"Accuracy: %0.5f\" % accuracy)\n",
    "assert accuracy > 0.65, \"we need more accuracy!\"\n",
    "print(\"Great job!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cEOheIsyzvDz"
   },
   "outputs": [],
   "source": [
    "def draw_results(question, possible_answers, predicted_index, correct_indices):\n",
    "    print(\"Q:\", question, end='\\n\\n')\n",
    "    for i, answer in enumerate(possible_answers):\n",
    "        print(\"#%i: %s %s\" % (i, '[*]' if i == predicted_index else '[ ]', answer))\n",
    "    \n",
    "    print(\"\\nVerdict:\", \"CORRECT\" if predicted_index in correct_indices else \"INCORRECT\", \n",
    "          \"(ref: %s)\" % correct_indices, end='\\n' * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LYJS6w6kzvD0"
   },
   "outputs": [],
   "source": [
    "for i in [1, 100, 1000, 2000, 3000, 4000, 5000]:\n",
    "    draw_results(test.iloc[i].question, test.iloc[i].options,\n",
    "                 predicted_answers[i], test.iloc[i].correct_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9R8hFOwPzvD1"
   },
   "outputs": [],
   "source": [
    "question = \"What is my name?\" # your question here!\n",
    "possible_answers = [\n",
    "    <...> \n",
    "    # ^- your options. \n",
    "]\n",
    "predicted answer = select_best_answer(question, possible_answers)\n",
    "\n",
    "draw_results(question, possible_answers,\n",
    "             predicted_answer, [0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "collapsed": true,
    "id": "RAmhaEaFzvD2"
   },
   "source": [
    "### Bonus tasks\n",
    "\n",
    "There are many ways to improve our question answering model. Here's a bunch of things you can do to increase your understanding and get bonus points.\n",
    "\n",
    "\n",
    "### 0. Fine-tuning (3+ pts)\n",
    "This time our dataset is fairly small. We can improve the training procedure by starting with a pre-trained model.\n",
    "* The simplest option is to use pre-trained embeddings. See previous weeks for that.\n",
    "* A harder (but better) alternative is to use a pre-trained sentence encoder. Consider [InferSent](https://github.com/facebookresearch/InferSent), Universal Sentence Encoder or ELMO.\n",
    "\n",
    "\n",
    "### 1.  Hard Negatives (3+ pts)\n",
    "\n",
    "Not all wrong answers are equally wrong. As the training progresses, _most negative examples $a^-$ will be to easy._ So easy in fact, that loss function and gradients on such negatives is exactly __0.0__. To improve training efficiency, one can __mine hard negative samples__.\n",
    "\n",
    "Given a list of answers,\n",
    "* __Hard negative__ is the wrong answer with highest similarity with question,\n",
    "\n",
    "$$a^-_{hard} = \\underset {a^-} {argmax} \\space sim[V_q(q), V_a(a^-)]$$\n",
    "\n",
    "* __Semi-hard negative__ is the one with highest similarity _among wrong answers that are farther than positive one. This option is more useful if some wrong answers may actually be mislabelled correct answers.\n",
    "\n",
    "* One can also __sample__ negatives proportionally to $$P(a^-_i) \\sim e ^ {sim[V_q(q), V_a(a^-_i)]}$$\n",
    "\n",
    "\n",
    "The task is to implement at least __hard negative__ sampling and apply it for model training.\n",
    "\n",
    "\n",
    "### 2. Bring Your Own Model (3+ pts)\n",
    "In addition to Universal Sentence Encoder, one can also train a new model.\n",
    "* You name it: convolutions, RNN, self-attention\n",
    "* Use pre-trained ELMO or FastText embeddings\n",
    "* Monitor overfitting and use dropout / word dropout to improve performance\n",
    "\n",
    "__Note:__ if you use ELMO please note that it requires tokenized text while USE can deal with raw strings. You can tokenize data manually or use tokenized=True when reading dataset.\n",
    "\n",
    "\n",
    "* hard negatives (strategies: hardest, hardest farter than current, randomized)\n",
    "* train model on the full dataset to see if it can mine answers to new questions over the entire wikipedia. Use approximate nearest neighbor search for fast lookup.\n",
    "\n",
    "\n",
    "### 3. Search engine (3+ pts)\n",
    "\n",
    "Our basic model only selects answers from 2-5 available sentences in paragraph. You can extend it to search over __the whole dataset__. All sentences in all other paragraphs are viable answers.\n",
    "\n",
    "The goal is to train such a model and use it to __quickly find top-10 answers from the whole set__.\n",
    "\n",
    "* You can ask such model a question of your own making - to see which answers it can find among the entire training dataset or even the entire wikipedia.\n",
    "* Searching for top-K neighbors is easier if you use specialized methods: [KD-Tree](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html) or [HNSW](https://github.com/nmslib/hnswlib). \n",
    "* This task is much easier to train if you use hard or semi-hard negatives. You can even find hard negatives for one question from correct answers to other questions in batch - do so in-graph for maximum efficiency. See [1.] for more details.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "zAxDxnGrzvDr",
    "RAmhaEaFzvD2"
   ],
   "name": "seminar_pytorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
