{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core import display\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#If you are running on a server, launch xvfb to record game videos\n",
    "#Please make sure you have xvfb installed\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С теорией могут помочь эти слайды: http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_5_actor_critic_pdf.pdf\n",
    "\n",
    "А могут и не помочь."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Мастерами кунг-фу не рождаются\n",
    "\n",
    "Сегодня мы будем играть в Atari [KungFuMaster](https://gym.openai.com/envs/KungFuMaster-v0/), на этот раз используя рекуррентные сетки.\n",
    "\n",
    "![kung-fu-master](https://lh3.googleusercontent.com/fzmeaDZPcTJqlrdA_NMhXOFkafTiM5JnBxUkYdgH_FlAjoCVWYmGbxia16MwnIpu1g=w412-h220-rw)\n",
    "\n",
    "Эта игра уже намного сложнее удержания палки в вертикальном положении. Состояние — это RGB картинка монитора (трёхмерный массив размера 210x160x3), а на выбор у вас есть целых 14 действий — разные перемещения, удары, прыжки и прочее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: atari-py in /home/kikos/anaconda3/lib/python3.5/site-packages (0.1.7)\r\n",
      "Requirement already satisfied: six in /home/kikos/anaconda3/lib/python3.5/site-packages (from atari-py) (1.11.0)\r\n",
      "Requirement already satisfied: numpy in /home/kikos/anaconda3/lib/python3.5/site-packages (from atari-py) (1.16.2)\r\n"
     ]
    }
   ],
   "source": [
    "# убедитесь, что atari_util.py находится в той же директории,\n",
    "# что и эта тетрадка, а также поставьте atari-py:\n",
    "!pip install atari-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (1, 42, 42)\n",
      "Num actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from atari_util import PreprocessAtari\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v0\")\n",
    "    # размерность картинки слишком большая: \n",
    "    # давайте обрежем её и перегоним в чб:\n",
    "    env = PreprocessAtari(env, height=42, width=42,\n",
    "                          crop = lambda img: img[60:-30, 15:],\n",
    "                          color=False, n_frames=1)\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "obs_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"Observation shape:\", obs_shape)\n",
    "print(\"Num actions:\", n_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s = env.reset()\n",
    "for _ in range(100):\n",
    "    s, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.title('Исходное изображение')\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "plt.show()\n",
    "\n",
    "plt.title('Что видит агент')\n",
    "plt.imshow(s.reshape([42,42]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Частично-наблюдаемые MDP\n",
    "\n",
    "Наша игра — это на самом деле POMDP: агент знает тайминги, когда враги спавнятся и двигаются, но понять это по одной картинке он не может — ему нужна какая-то память для этого.\n",
    "\n",
    "Нам нужна какая-то сеть, которая использует память рекуррентной сети. Например, такая:\n",
    "\n",
    "<img src='https://github.com/yandexdataschool/Practical_RL/blob/spring19/week08_pomdp/img1.jpg?raw=true' width='500px'> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# [batch, channel, w, h] -> [batch, units]\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRecurrentAgent(nn.Module):\n",
    "    def __init__(self, obs_shape, n_actions, reuse=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv0 = nn.Conv2d(1, 32, kernel_size=(3,3), stride=(2,2))\n",
    "        self.conv1 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=(2,2))\n",
    "        self.conv2 = nn.Conv2d(32, 32, kernel_size=(3,3), stride=(2,2))\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        self.hid = nn.Linear(512, 128)\n",
    "        self.rnn = nn.LSTMCell(128, 128)\n",
    "\n",
    "        self.logits = nn.Linear(128, n_actions)\n",
    "        self.state_value = nn.Linear(128, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, prev_state, obs_t):\n",
    "        \"\"\"\n",
    "        Принимает предыдущее состояние (память) и наблюдени,\n",
    "        возвращает следующее состояние и пару из политики (actor) и оценки состояния (critic) \n",
    "        \"\"\"\n",
    "        obs = self.flatten(self.conv2(self.conv1(self.conv0(obs_t))))\n",
    "        \n",
    "        \n",
    "        new_state = self.rnn(self.hid(obs), prev_state)\n",
    "        logits = self.logits(new_state[0])\n",
    "        state_value = self.state_value(new_state[0])\n",
    "        \n",
    "        return new_state, (logits, state_value)\n",
    "    \n",
    "    def get_initial_state(self, batch_size):\n",
    "        \"\"\"Возвращает память агента в начале игры.\"\"\"\n",
    "        return torch.zeros((batch_size, 128)), torch.zeros((batch_size, 128))\n",
    "    \n",
    "    def sample_actions(self, agent_outputs):\n",
    "        \"\"\"Делает случайное действие, в соответствие с предсказанными вероятностями.\"\"\"\n",
    "        logits, state_values = agent_outputs\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        return torch.multinomial(probs, 1)[:, 0].data.numpy()\n",
    "    \n",
    "    def step(self, prev_state, obs_t):\n",
    "        \"\"\"Подобно forward, но obs_t это не торчевый тензор.\"\"\"\n",
    "        obs_t = torch.tensor(np.array(obs_t), dtype=torch.float32)\n",
    "        (h, c), (l, s) = self.forward(prev_state, obs_t)\n",
    "        return (h.detach(), c.detach()), (l.detach(), s.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_parallel_games = 5\n",
    "gamma = 0.99\n",
    "\n",
    "agent = SimpleRecurrentAgent(obs_shape, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action logits:\n",
      " tensor([[ 0.0134,  0.0767,  0.0169,  0.0260,  0.0150, -0.0011, -0.0327,  0.0484,\n",
      "         -0.0133,  0.0651,  0.0161,  0.0645, -0.0108,  0.0189]])\n",
      "State values:\n",
      " tensor([[0.0414]])\n"
     ]
    }
   ],
   "source": [
    "state = [env.reset()]\n",
    "_, (logits, value) = agent.step(agent.get_initial_state(1), state)\n",
    "print(\"Action logits:\\n\", logits)\n",
    "print(\"State values:\\n\", value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play!\n",
    "\n",
    "Напишем функцию, которая меряет средний reward агента:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_games=1):\n",
    "    \"\"\"Играет игру от начала до конца и возвращает награды на каждом шаге.\"\"\"\n",
    "\n",
    "    game_rewards = []\n",
    "    for _ in range(n_games):\n",
    "        observation = env.reset()\n",
    "        prev_memories = agent.get_initial_state(1)\n",
    "\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            new_memories, readouts = agent.step(prev_memories, observation[None, ...])\n",
    "            action = agent.sample_actions(readouts)\n",
    "\n",
    "            observation, reward, done, info = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "            prev_memories = new_memories\n",
    "            if done: break\n",
    "                \n",
    "        game_rewards.append(total_reward)\n",
    "    return game_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200.0, 500.0, 300.0]\n"
     ]
    }
   ],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "rw = evaluate(agent, env_monitor, n_games=3,)\n",
    "env_monitor.close()\n",
    "print(rw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./kungfu_videos/openaigym.video.0.1174.video000000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# видосик\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./kungfu_videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Параллельные игры\n",
    "\n",
    "Введем EnvPool — это такая абстракция для управления множественными средами:\n",
    "\n",
    "![](https://github.com/yandexdataschool/Practical_RL/blob/spring19/week08_pomdp/img2.jpg?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env_pool import EnvPool\n",
    "pool = EnvPool(agent, make_env, n_parallel_games)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/yandexdataschool/Practical_RL/blob/spring19/week08_pomdp/img3.jpg?raw=true)\n",
    "\n",
    "*Роллаут* это просто последовательность наблюдений, действий и наград, которые произошли последовательно друг за другом. Обратите внимание, что это не то же самое, что сессия:\n",
    "\n",
    "* Первое состояние не обязательно начальное для среды.\n",
    "* Последнее состояние не обязательно терминальное.\n",
    "* По соображениям эффективности, мы будем сэмплировать несколько параллельных роллаутов одновременно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделать 10 шагов в каждой из n_parallel_games игр\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions shape: (5, 10)\n",
      "Rewards shape: (5, 10)\n",
      "Mask shape: (5, 10)\n",
      "Observations shape:  (5, 10, 1, 42, 42)\n"
     ]
    }
   ],
   "source": [
    "print(\"Actions shape:\", rollout_actions.shape)\n",
    "print(\"Rewards shape:\", rollout_rewards.shape)\n",
    "print(\"Mask shape:\", rollout_mask.shape)\n",
    "print(\"Observations shape: \",rollout_obs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Виды RL-я\n",
    "\n",
    "Value-based:\n",
    "* Оценивают value состояния — какая там ожидаемая награда\n",
    "* Политики нет, точнее она неявная — идём туда, где больше value. При обучении пользуемся $\\epsilon$-greedy.\n",
    "\n",
    "Policy-based:\n",
    "* Оценивают оптимальную policy.\n",
    "* Value function нет.\n",
    "* (Мы делали ровно это в прошлый раз.)\n",
    "\n",
    "Actor-critic:\n",
    "* Оцениваем и policy, и value (независимо обучаем две сетки).\n",
    "* Это позволяет сильно улучшить сходимость policy, потому что с помощью value-функции можно нормировать реворды из состояния, что уменьшит дисперсию градиентов.\n",
    "\n",
    "![actor-critic](https://cs.wmich.edu/~trenary/files/cs5300/RLBook/figtmp34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция потерь\n",
    "\n",
    "Наш лосс будет состоять из трёх компонент:\n",
    "\n",
    "* **Policy loss:**\n",
    " $$ \\hat J = {1 \\over T} \\cdot \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s,a) $$\n",
    "  * Напоминание: сама по себе эта функция не имеет смысла, и была выведена так, что\n",
    "  * $ \\nabla \\hat J = {1 \\over N} \\cdot \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s,a) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n",
    "  * То есть если мы максимизируем (не перепутайте тут знак) $\\hat J$ градиентным спуском, то мы максимизируем ожидаемый ревард.\n",
    "\n",
    "\n",
    "* **Value loss:**\n",
    "  $$ L_{td} = {1 \\over T} \\cdot \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n",
    "  * Если мы будем минимизировать его, то $V(s)$ сойдётся к $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n",
    "\n",
    "\n",
    "* **Энтропийная регуляризация** — вы это уже видели:\n",
    "  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n",
    "  * Если максимизировать энтропию, то агент не захочет предсказывать нулевые вероятности для действий (a.k.a. exploration)\n",
    "  \n",
    "  \n",
    "Так что мы будем оптимизировать какую-то линейную комбинацию $L_{td}$, $- \\hat J$ и $-H$\n",
    "\n",
    "**Кстати:** раз мы при обучении рассматриваем роллауты на $T$ последовательных шагов, то мы можем бесплатно использовать более точные формулы для $A(s_t, a_t)$:\n",
    "  * Последний шаг: $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s) $\n",
    "  * Предпоследний шаг: $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s) $\n",
    "  * ...И так далее. Так агент обучается намного быстрее, потому что он меньше зависит от шумной оценки $V$ и больше на сами награды.\n",
    "  * По этой причине хорошей идеей будет увеличить rollout_len (до >=20). Ещё его лучше увеличивать с течением времени, но не обязательно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    y_tensor = torch.tensor(y, dtype=torch.int64).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-5)\n",
    "\n",
    "def train_on_rollout(states, actions, rewards, is_not_done, prev_memory_states, gamma = 0.99):\n",
    "    \"\"\"\n",
    "    Берет роллаут -- последовательность состояний, действий и наград, полученных из generate_session.\n",
    "    Обновляет веса агента через policy gradient.\n",
    "    Менять параметры Adam-а не рекомендуется.\n",
    "    \"\"\"\n",
    "    \n",
    "    # сконвертируем всё в torch.tensor\n",
    "    states = torch.tensor(np.array(states), dtype=torch.float32)   # [batch_size, time, c, h, w]\n",
    "    actions = torch.tensor(np.array(actions), dtype=torch.int64)   # [batch_size, time]\n",
    "    rewards = torch.tensor(np.array(rewards), dtype=torch.float32) # [batch_size, time]\n",
    "    is_not_done = torch.tensor(is_not_done.astype('float32'), dtype=torch.float32)  # [batch_size, time]\n",
    "    rollout_length = rewards.shape[1] - 1\n",
    "\n",
    "    # теперь нужно посчитать логиты, вероятности и лог-вероятности\n",
    "    # больше для лосса нам ничего не нужно от модели\n",
    "    \n",
    "    memory = [m.detach() for m in prev_memory_states]\n",
    "    \n",
    "    logits = []\n",
    "    state_values = []\n",
    "    for t in range(rewards.shape[1]):\n",
    "        obs_t = states[:, t]\n",
    "        \n",
    "        # вычислите моделью logits_t и values_t.\n",
    "        # и зааппендьте их к спискам logits и state_values\n",
    "        \n",
    "        memory, (logits_t, values_t) = agent(memory, obs_t)\n",
    "        \n",
    "        logits.append(logits_t)\n",
    "        state_values.append(values_t)\n",
    "        \n",
    "    logits = torch.stack(logits, dim=1)\n",
    "    state_values = torch.stack(state_values, dim=1)\n",
    "    probas = torch.softmax(logits, dim=2)\n",
    "    logprobas = torch.log_softmax(logits, dim=2)\n",
    "        \n",
    "    # выбираем лог-вероятности для реальных действий -- log pi(a_i|s_i)\n",
    "    actions_one_hot = to_one_hot(actions, n_actions).view(\n",
    "        actions.shape[0], actions.shape[1], n_actions)\n",
    "    logprobas_for_actions = torch.sum(logprobas * actions_one_hot, dim = -1)\n",
    "    \n",
    "    # Теперь посчитайте две основные компоненты лосса:\n",
    "    # 1) Policy gradient\n",
    "    # Примечание: не забываейте делать .detach() для advantage.\n",
    "    # Ещё лучше использовать mean, а не sum, чтобы lr не масштабировать.\n",
    "    # Можно исползовать тут циклы, если хотите.\n",
    "    J_hat = 0  # посчитаем ниже\n",
    "    \n",
    "    # 2) Temporal difference MSE\n",
    "    value_loss = 0  # посчитаем ниже\n",
    "    \n",
    "    cumulative_returns = state_values[:, -1].detach()\n",
    "    for t in reversed(range(rollout_length)):\n",
    "        r_t = rewards[:, t]                        # текущие reward-ы\n",
    "        V_t = state_values[:, t]                   # value текущих состоияний\n",
    "        V_next = state_values[:, t + 1].detach()   # value следующих состояний\n",
    "        logpi_a_s_t = logprobas_for_actions[:, t]  # вероятности сделать нужное действие\n",
    "        \n",
    "        # G_t = r_t + gamma * G_{t+1}, как в прошлый раз на reinforce\n",
    "        cumulative_returns = G_t = r_t + gamma * cumulative_returns\n",
    "        \n",
    "        # Посчитайте MSE для V(s)\n",
    "        value_loss += # ...\n",
    "        \n",
    "        # посчитайте advantage A(s_t, a_t), используя cumulative returns и V(s_t) в качестве бейзлайна\n",
    "        advantage = # ...\n",
    "        advantage = advantage.detach()\n",
    "        \n",
    "        # посчитаем весь policy loss (-J_hat).\n",
    "        J_hat += <YOUR CODE>\n",
    "    \n",
    "    entropy_reg = # compute entropy regularizer\n",
    "    \n",
    "    # усредним всё это дело с какими-то весами\n",
    "    loss = -J_hat / rollout_length +\\\n",
    "           value_loss / rollout_length +\\\n",
    "           -0.01 * entropy_reg\n",
    "    \n",
    "    # сделайте шаг против градиента\n",
    "    # ...\n",
    "    \n",
    "    return loss.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# проверим работу\n",
    "memory = list(pool.prev_memory_states)\n",
    "rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "\n",
    "train_on_rollout(rollout_obs, rollout_actions, rollout_rewards, rollout_mask, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import trange\n",
    "from pandas import DataFrame\n",
    "moving_average = lambda x, **kw: DataFrame({'x':np.asarray(x)}).x.ewm(**kw).mean().values\n",
    "\n",
    "rewards_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(15000):  \n",
    "    \n",
    "    memory = list(pool.prev_memory_states)\n",
    "    rollout_obs, rollout_actions, rollout_rewards, rollout_mask = pool.interact(10)\n",
    "    train_on_rollout(rollout_obs, rollout_actions, rollout_rewards, rollout_mask, memory)    \n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        rewards_history.append(np.mean(evaluate(agent, env, n_games=1)))\n",
    "        clear_output(True)\n",
    "        plt.plot(rewards_history, label='rewards')\n",
    "        plt.plot(moving_average(np.array(rewards_history),span=10), label='rewards ewma@10')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        if rewards_history[-1] >= 10000:\n",
    "            print(\"Yellow belt awarded\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Агент сейчас заперт в бесконечном цикле жестокости и насилий.\n",
    "\n",
    "### Дебаг\n",
    "\n",
    "Награды за сессию должны в целом идти в верх, но это нормально, если они будут колебаться (очень сильно). Так же нормально, что агент ничего солидного не выучит после 10к первых итераций. Что-то не так идет только тогда, реворд нулевой и не поднимается спустя 2-3 эвалов подряд.\n",
    "\n",
    "Мы используем policy-based метод, и тут полезно смотреть на энтропию политики (та штука, которую мы использовали как регуляризацию). Если она становится слишком мала ($< 0.1$) до того момента, когда ваш агент получит желтый пояс, то что-то идет не так.\n",
    "\n",
    "Если что-то не так, то проверьте в первую очередь следующее:\n",
    "* Какой-то баг в энтропии: $ - \\sum p(a_i) \\cdot log p(a_i) $\n",
    "* Сеть слишком быстро сходится к чему-то неоптимальному. Увеличьте коэффициент регуляризации.\n",
    "* Взрывающиеся градиенты — обрежьте градиенты (`gradient_clip`) и, возможно, используйте нейронку поменьше.\n",
    "\n",
    "При дебаге полезно запустить `logits, values = agent.step(batch_states)` и глазами посмотреть на логиты и value: возможно, там будут какие-нибудь NaN-ы, безумно большие числа или нули. Отловите этот момент как только это случилось и попытайтесь понять, что пошло не так."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Видосик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_monitor = gym.wrappers.Monitor(env, directory=\"kungfu_videos\", force=True)\n",
    "final_rewards = evaluate(agent, env_monitor, n_games=20,)\n",
    "env_monitor.close()\n",
    "print(\"Final mean reward\", np.mean(final_rewards))\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./kungfu_videos/\")))\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./kungfu_videos/\"+video_names[-1]))  # убедитесь, что файл правильный"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
