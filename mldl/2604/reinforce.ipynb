{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE\n",
    "\n",
    "Пусть $X$ это какая-то случайная величина с известным распределением $p_\\theta(X)$, где $\\theta$ — это параметры модели. Определим функцию награды $J(\\theta)$ как\n",
    "\n",
    "$$ J(\\theta) = E[f(X)] = \\int_x f(x) p(x) \\; dx $$\n",
    "\n",
    "для произвольной функции $f$, не зависящей от $\\theta$.\n",
    "\n",
    "*Policy gradient* (дословно, градиент стратегии) — это, интуитивно, то направление, куда нужно двигать параметры модели, чтобы функция награды увеличивалась. Алгоритм REINFORCE (почему его всегда пишут капсом автор не знает) на каждом шаге просто определяет policy gradient $\\nabla_\\theta J(\\theta)$ и изменяет параметры в его сторону с каким-то learning rate-ом.\n",
    "\n",
    "**Зачем это надо**, если есть обычный градиентный спуск? Через policy gradient и reinforcement learning вообще можно оптимизировать более общий класс функций — хоть дискретные (например, можно BLEU для перевода напрямую максимизировать) и даже невычислимые (какие-нибудь субъективные оценки асессоров).\n",
    "\n",
    "В частности, в таком ключе можно описать игры (для простоты, однопользовательские): есть какие-то награды за совершение каких-то действий (для шахмат: +1 за победу, 0 за ничью, -1 за поражение; для тетриса: +0.1 за «выживание» ещё одну секунду, 1 за удаление слоя, 0 за проигрыш) и нам нужно подобрать такие параметры модели, чтобы максимизировать ожидаемую сумму наград за действия, которые мы совершили, то есть в точности $J(\\theta)$.\n",
    "\n",
    "Теперь немного математики: как нам найти этот $\\nabla_\\theta J(\\theta)$? Оказывается, мы можем выразить его ожидание, а тогда приблизительный градиент можно будет находить сэмплированием и усреднением градиентов — так же, как мы обычно обучаем нейросети на батчах.\n",
    "\n",
    "$$ J(\\theta) = E[f(X)] = \\int_x f(x) p(x) \\; dx = \\int_x f(x) p(x) \\nabla_\\theta \\log p(x) \\; dx = E[f(x) \\nabla_\\theta \\log p(x)] $$\n",
    "\n",
    "Переход между 2 и 3 верен, потому что $\\nabla \\log p(x) = \\frac{\\nabla p(x)}{p(x)}$ (просто подставьте и $p(x)$ сократится). Это называют log-derivative trick.\n",
    "\n",
    "Мы научились получить приблизительный градиент через сэмплирование. Давайте теперь что-нибудь обучим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `CartPole-v0`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Про задачу можно почитать тут: https://gym.openai.com/envs/CartPole-v0/. Tl;dr: есть вертикально стоящая палка на подвижной платформе; нужно двигать платформу так, чтобы палка не упала.\n",
    "\n",
    "<img width='500px' src='https://preview.redd.it/sqjzj2cgnpt21.gif?overlay-align=bottom,left&overlay-pad=8,16&crop=1200:628.272251309,smart&overlay-height=0.10&overlay=%2Fv9vyirk6hl221.png%3Fs%3Db466421949eb723078743745ce6421609d7a9c66&width=1200&height=628.272251309&s=ba84ac5a9c14946456808c15f2754cb7369b8de9'>\n",
    "\n",
    "OpenAI в 2016-м году выпустили `gym` — библиотечку для абстрагирования RL-ных сред от алгоритма. Есть абстрактная *среда* (`env`), в ней есть какие-то *состояния* (`state`), из каждого состояния есть какой-то фиксированный набо *действий* (`action`), ведущих (возможно, с какими-то вероятностями) в другие состояния, и за разные действия в разных состояниях дается какая-то *награда* (`reward`). Как конкретно устроена игра, нам думать не нужно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сеть\n",
    "\n",
    "Для REINFORCE вам нужна модель, которая берёт на вход состояние (каким-то образом закодированное) и возвращает вероятностное распределение действий в нём.\n",
    "\n",
    "Старайтесь не перемудрить — в общем случае сети для RL могут быть [довольно сложными](https://d4mucfpksywv.cloudfront.net/research-covers/openai-five/network-architecture.pdf), но CartPole не стоит того, чтобы писать глубокие архитектуры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = nn.Sequential(\n",
    "    nn.Linear(4, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 5),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(5, 2),\n",
    "    # ...\n",
    "    # какие-нибудь релушки и линеары\n",
    "    # ...\n",
    "    nn.LogSoftmax(dim=1)  # важно, что на выходе должны быть не вероятности, а логиты\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenAI Gym работает с numpy, а не напрямую с фреймворками. Для удобства, напишите функцию-обёртку, которая принимает состояния (`numpy array` размера `[batch, state_shape]`) и возвращает вероятности (размера `[batch, n_actions]]`, должны суммироваться в единицу)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(states):\n",
    "    x = torch.tensor(states, dtype=torch.float64).float()\n",
    "    x = agent(x)\n",
    "    return x, torch.exp(x)\n",
    "    # сконвертируйте состояния в тензор\n",
    "    # вычислите логиты\n",
    "    # вызовите софтмакс, чтобы получить веряотности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-1.0189, -0.4478],\n",
      "        [-1.0160, -0.4495],\n",
      "        [-1.0174, -0.4487],\n",
      "        [-1.0181, -0.4483],\n",
      "        [-1.0154, -0.4498]], grad_fn=<LogSoftmaxBackward>), tensor([[0.3610, 0.6390],\n",
      "        [0.3620, 0.6380],\n",
      "        [0.3615, 0.6385],\n",
      "        [0.3613, 0.6387],\n",
      "        [0.3622, 0.6378]], grad_fn=<ExpBackward>))\n"
     ]
    }
   ],
   "source": [
    "test_states = np.array([env.reset() for _ in range(5)])\n",
    "test_probas = predict_proba(test_states)\n",
    "print(test_probas)\n",
    "# assert isinstance(test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n",
    "# assert tuple(test_probas.shape) == (test_states.shape[0], n_actions), \"wrong output shape: \" +  str(np.shape(test_probas))\n",
    "# assert np.allclose(np.sum(test_probas, axis = 1), 1), \"probabilities do not sum to 1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестовый прогон\n",
    "\n",
    "Хоть наша модель не обучена, её уже можно использовать, чтобы играть в произвольной среде."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000, draw=False):\n",
    "    \"\"\" \n",
    "    Играет одну сессию REINFORCE-агентом и возвращает последовательность состояний,\n",
    "    действий и наград, которые потом будут использоваться при обучении.\n",
    "    \"\"\"\n",
    "    \n",
    "    # тут будем хранить сессию\n",
    "    states, actions, rewards, log_probas, probas = [],[],[], torch.tensor([]), torch.tensor([])\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "#         print(t)\n",
    "        if draw:\n",
    "            env.render(\"rgb-array\")\n",
    "        # вероятности следующих действий, aka p(a|s)\n",
    "        log_action_probas, action_probas = predict_proba(np.array([s]))\n",
    "        log_probas = torch.cat([log_probas, log_action_probas])\n",
    "        probas = torch.cat([probas, action_probas])\n",
    "        # сэмплируйте оттуда действие (посказка: np.random.choice)\n",
    "        a = np.random.choice([0, 1], p=action_probas[0].detach().numpy())\n",
    "        \n",
    "        new_s, r, done, info = env.step(a)\n",
    "        \n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        \n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "    return states, actions, rewards, log_probas, probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# протестируйте\n",
    "states, actions, rewards, logprobas, probas = generate_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing cumulative rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Принимает массив ревардов и возвращает discounted массив по следующей формуле:\n",
    "    \n",
    "        G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "    \n",
    "    Тут нет ничего сложного -- итерируйтесь от последнего до первого\n",
    "    и насчитывайте G_t = r_t + gamma*G_{t+1} рекуррентно.\n",
    "    \"\"\"\n",
    "    G = np.zeros(len(rewards))\n",
    "    G[-1] = rewards[-1]\n",
    "    for i in range(len(rewards) - 2, -1, -1):\n",
    "        G[i] = rewards[i] + gamma * G[i + 1]\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вроде норм\n"
     ]
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)\n",
    "assert len(get_cumulative_rewards(list(range(100)))) == 100\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9),\n",
    "                   [1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, -2, 3, -4, 0], gamma=0.5),\n",
    "                   [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n",
    "assert np.allclose(get_cumulative_rewards([0, 0, 1, 2, 3, 4, 0], gamma=0),\n",
    "                   [0, 0, 1, 2, 3, 4, 0])\n",
    "print(\"Вроде норм\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and updates\n",
    "\n",
    "Вспомним, что нам нужно оптимизировать\n",
    "\n",
    "$$ J \\approx  { 1 \\over N } \\sum  _{s_i,a_i} \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "\n",
    "Используя REINFORCE, нам в алгоритме по сути нужно максимизировать немного другую функцию:\n",
    "\n",
    "$$ \\hat J \\approx { 1 \\over N } \\sum  _{s_i,a_i} log \\pi_\\theta (a_i | s_i) \\cdot G(s_i,a_i) $$\n",
    "\n",
    "Когда мы будем вычислять её градиент, мы получим в точности policy gradient из REINFORCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y_tensor, n_dims=None):\n",
    "    \"\"\" Конвертирует целочисленный вектор в one-hot матрицу. \"\"\"\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# тут определите оптимизатор для модели\n",
    "# например, Adam с дефолтными параметрами\n",
    "optimiser = torch.optim.Adam(agent.parameters())\n",
    "\n",
    "def train_on_session(states, actions, rewards, logprobas, probas, gamma = 0.99):\n",
    "    \n",
    "    states = torch.tensor(states, dtype=torch.float32)\n",
    "#     logprobas = torch.tensor(logprobas, dtype=torch.float32)\n",
    "#     probas = torch.tensor(probas, dtype=torch.float32)\n",
    "    actions = torch.tensor(actions, dtype=torch.int32)\n",
    "    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n",
    "    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32)\n",
    "    \n",
    "    # выберем и просуммируем лог-вероятности только для выбранных действий\n",
    "    logprobas_for_actions = torch.sum(logprobas * to_one_hot(actions), dim=1)\n",
    "    \n",
    "    J_hat = 1 / len(states) * torch.dot(logprobas_for_actions, cumulative_returns) # формула для REINFORCE\n",
    "    \n",
    "#     # опционально: энтропийная регуляризация\n",
    "    entropy_reg = -torch.sum(torch.exp(logprobas_for_actions) * logprobas_for_actions) / len(logprobas_for_actions) # вычислите среднюю энтропию вероятностей; не забудьте знак!\n",
    "    \n",
    "    loss = -J_hat + 0.1 * entropy_reg\n",
    "    \n",
    "    loss.backward()\n",
    "    optimiser.step()\n",
    "    agent.zero_grad()\n",
    "#     # шагните в сторону градиента\n",
    "#     # ....\n",
    "    \n",
    "#     # верните ревард сессии, чтобы потом их печатать\n",
    "    return np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Само обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:17.610\n",
      "mean reward:19.100\n",
      "mean reward:22.340\n",
      "mean reward:22.080\n",
      "mean reward:21.720\n",
      "mean reward:27.250\n",
      "mean reward:24.780\n",
      "mean reward:23.190\n",
      "mean reward:29.810\n",
      "mean reward:34.060\n",
      "mean reward:40.530\n",
      "mean reward:48.730\n",
      "mean reward:53.490\n",
      "mean reward:64.080\n",
      "mean reward:93.080\n",
      "mean reward:85.600\n",
      "mean reward:93.200\n",
      "mean reward:198.780\n",
      "mean reward:191.280\n",
      "mean reward:145.420\n",
      "mean reward:200.670\n",
      "mean reward:291.230\n",
      "mean reward:158.160\n",
      "mean reward:187.890\n",
      "mean reward:161.720\n",
      "mean reward:212.590\n",
      "mean reward:279.560\n",
      "mean reward:317.810\n",
      "mean reward:133.900\n",
      "mean reward:360.950\n",
      "mean reward:487.390\n",
      "mean reward:539.630\n",
      "Победа!\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    rewards = [train_on_session(*generate_session()) for _ in range(100)]\n",
    "    print (\"mean reward:%.3f\"%(np.mean(rewards)))\n",
    "\n",
    "    if np.mean(rewards) > 500:\n",
    "        print (\"Победа!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Видосик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session(draw=True) for _ in range(1)]\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"./videos/openaigym.video.0.18573.video000000.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
